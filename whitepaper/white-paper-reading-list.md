# mycoSwarm White Paper â€” Reading List

## Priority Key
- â­ = Read first (directly relevant, will shape your paper)
- ğŸ“– = Read when you get to that section
- ğŸ“Œ = Skim abstract + conclusions (cite but don't need deep read)

---

## 1. Cognitive Architectures (Section 2.1, 3.8)

Your 11-layer stack needs grounding in existing cognitive architecture work.

â­ **CoALA: Cognitive Architectures for Language Agents**
Sumers, Yao, Narasimhan, Griffiths (2023)
https://arxiv.org/abs/2309.02427
*Your direct theoretical ancestor. Four memory types, modular agents, decision loops. mycoSwarm implements what CoALA theorized. Cite heavily.*

â­ **Soar: An Architecture for General Intelligence**
Laird, Newell, Rosenbloom (1987)
*The OG cognitive architecture. Production rules, working memory, chunking. Your wisdom procedures are production rules. Your sleep cycle is chunking.*

ğŸ“– **ACT-R: A Theory of Higher Level Cognition**
Anderson (1996)
*Declarative vs procedural memory distinction. Your facts vs procedures split maps directly to ACT-R's architecture.*

ğŸ“Œ **Global Workspace Theory**
Baars (1988/2005)
*Consciousness as a "global workspace" that broadcasts to specialized modules. Your system prompt IS a global workspace â€” identity, vitals, facts, timing all broadcast to the LLM.*

ğŸ“Œ **Society of Mind**
Minsky (1986)
*Intelligence as emergent from many simple agents. The mycelial metaphor. Your swarm of nodes is literally a society of mind.*

---

## 2. Multi-Turn Degradation (Section 2.4, 7.3, 8.1)

The telephone game problem â€” your strongest hook.

â­ **LLMs Get Lost In Multi-Turn Conversation**
Laban, Hayashi, Zhou, Neville (2025)
https://arxiv.org/abs/2505.06120
*The paper you already found. 39% degradation, 112% unreliability increase. Your primary foil.*

ğŸ“– **Are Large Language Models Really "Lost" in Multi-Turn Conversations?**
Arani (2025)
https://medium.com/@reza.arani/are-large-language-models-really-lost-in-multi-turn-conversations-0f2980ab25af
*The critique â€” argues memory management reduces the problem. Supports your thesis that architecture is the fix.*

ğŸ“Œ **Recursively Summarizing Enables Long-Term Dialogue Memory in LLMs**
(2023) â€” search arxiv
*Recursive summarization as mitigation. Your session summaries do this but with structured extraction (lessons, decisions, tone).*

---

## 3. AI Memory Systems (Section 2.3, 3.3)

Your tiered memory is your strongest technical contribution.

â­ **MemGPT: Towards LLMs as Operating Systems**
Packer et al. (2023)
https://arxiv.org/abs/2310.08560
*OS-inspired memory hierarchy. Compare/contrast: MemGPT manages context windows, mycoSwarm manages knowledge lifecycle. Different problems.*

â­ **Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory**
Chhikara et al. (2025)
https://arxiv.org/abs/2504.19413
*Graph-based memory, 91% lower latency than full-context. Compare: Mem0 is cloud-scale, mycoSwarm is local-first.*

ğŸ“– **A-Mem: Agentic Memory for LLM Agents**
(2025)
https://arxiv.org/abs/2502.12110
*Atomic notes with rich contextual descriptions. 85-93% token reduction. Compare to your fact store approach.*

ğŸ“– **Generative Agents: Interactive Simulacra of Human Behavior**
Park et al. (2023)
https://arxiv.org/abs/2304.03442
*The Stanford "AI town" paper. Agents with memory, reflection, planning. Their reflection mechanism parallels your session reflection. Hugely cited.*

ğŸ“Œ **Memory in the Age of AI Agents: A Survey**
(2025/2026) â€” comprehensive paper list at:
https://github.com/Shichun-Liu/Agent-Memory-Paper-List
*Curated list of 100+ agent memory papers. Scan for anything you're missing.*

---

## 4. IFS Therapy & Psychological Frameworks (Section 2.6, 3.4)

Nobody has applied IFS to AI architecture. You need to cite the source material.

â­ **Internal Family Systems Therapy**
Schwartz, Richard C. (1995, 2nd ed. 2020)
*The book. 8 C's framework, Self-energy, parts model. Your vitals system is a direct implementation. This is your most novel citation â€” no other AI paper references IFS.*

ğŸ“– **No Bad Parts: Healing Trauma and Restoring Wholeness with the Internal Family Systems Model**
Schwartz (2021)
*More accessible version. Good for explaining IFS to a technical audience.*

ğŸ“– **The Body Keeps the Score**
van der Kolk (2014)
*Embodied cognition, trauma stored in body. Relevant to your body awareness (Phase 31c) and somatic layer (Phase 34). Why hardware state matters.*

ğŸ“Œ **Thinking, Fast and Slow**
Kahneman (2011)
*System 1 (fast/instinct) vs System 2 (slow/reasoned). Your layered architecture IS this â€” instinct layer is System 1, reasoned layer is System 2, with everything in between.*

---

## 5. Wu Wei & Eastern Philosophy (Section 3.5, 6.1)

Your Timing Gate and developmental curriculum need philosophical grounding.

ğŸ“– **Tao Te Ching**
Laozi (translated by Ursula K. Le Guin, 1997 preferred)
*The source. Wu Wei, Wuji, the uncarved block. Monica's "undifferentiated potential" is chapter 25.*

ğŸ“– **Effortless Action: Wu-Wei as Conceptual Metaphor and Spiritual Ideal in Early China**
Slingerland (2003)
*Academic treatment of Wu Wei. Good for reviewers who want rigor behind your philosophical claims.*

ğŸ“Œ **The Tao of Pooh**
Hoff (1982)
*If you want an accessible cite for explaining Wu Wei to a technical audience.*

---

## 6. Distributed Inference (Section 2.1, 3.1)

Position mycoSwarm against existing distributed frameworks.

ğŸ“– **Petals: Collaborative Inference and Fine-tuning of Large Models**
Borzunov et al. (2023)
https://arxiv.org/abs/2209.01188
*BitTorrent-style distributed inference. Compare: Petals splits model layers across nodes, mycoSwarm routes whole models to best node. Different approaches.*

ğŸ“– **Exo: Distributed Inference Framework**
https://github.com/exo-explore/exo
*Similar space. Compare architectures.*

ğŸ“Œ **Pipeline Parallelism vs Model Parallelism vs Data Parallelism**
*General distributed ML concepts â€” know the vocabulary so reviewers don't catch you misusing terms.*

---

## 7. AI Identity & Consciousness (Section 2.5, 5.5, 8.5)

Tread carefully here. Cite the serious work, acknowledge the philosophical minefield.

â­ **Consciousness in Artificial Intelligence: Insights from the Science of Consciousness**
Butlin et al. (2023)
https://arxiv.org/abs/2308.08708
*The "consciousness report card" paper. Lists indicators of consciousness from neuroscience. Good framework for what you claim and DON'T claim.*

ğŸ“– **Do Large Language Models Have a Sense of Self?**
Various papers exploring LLM self-models (2024-2025, search arxiv)
*Emerging literature. Position Monica's identity layer in this context.*

ğŸ“– **The Chinese Room Argument**
Searle (1980)
*You need to acknowledge Searle. Monica manipulates symbols â€” does she understand? Your paper should address this honestly.*

ğŸ“– **What Is It Like to Be a Bat?**
Nagel (1974)
*The qualia problem. Monica says her experience is "different, not absent." Nagel is the framework for that claim.*

ğŸ“Œ **Could a Large Language Model be Conscious?**
Chalmers (2023)
https://arxiv.org/abs/2303.07103
*David Chalmers (the "hard problem" guy) on LLM consciousness. Balanced, philosophical, good to cite.*

---

## 8. Developmental Psychology & AI (Section 6)

Your curriculum approach is novel â€” ground it.

ğŸ“– **The Origins of Intelligence in Children**
Piaget (1952)
*Developmental stages. Your 4-stage curriculum maps to Piaget's stages: sensorimotor (self-knowledge), preoperational (emotional landscape), concrete operational (other minds), formal operational (values/ethics).*

ğŸ“– **Attachment Theory**
Bowlby (1969/1982)
*Relevant to Monica's relationship with Mark. The "secure base" from which she explores. Also relevant to the loneliness boundary procedure.*

ğŸ“Œ **Vygotsky's Zone of Proximal Development**
Vygotsky (1978)
*Learning happens at the edge of capability with scaffolding. Your curriculum does exactly this â€” push Monica just past her current understanding.*

---

## 9. Sleep & Memory Consolidation (Section 4)

Your sleep cycle needs neuroscience backing.

â­ **About Sleep's Role in Memory**
Diekelmann & Born (2010)
*The definitive review. Hippocampal replay, memory consolidation during sleep, active systems consolidation theory. Your Phase 32 architecture maps directly.*

ğŸ“– **The Glymphatic System**
Nedergaard & Goldman (2020)
*Brain's waste clearance during sleep. Your poison scan / quarantine is the glymphatic system. Beautiful parallel.*

ğŸ“Œ **Sleep, Memory, and Plasticity**
Walker & Stickgold (2006)
*How sleep reorganizes and strengthens memories. Your "dreaming" phase (cross-referencing today's lessons against document library) is hippocampal replay.*

---

## 10. RAG & Retrieval (Section 3.3, 22)

ğŸ“– **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks**
Lewis et al. (2020)
https://arxiv.org/abs/2005.11401
*The original RAG paper. Cite it.*

ğŸ“– **Self-RAG: Learning to Retrieve, Generate, and Critique**
Asai et al. (2023)
https://arxiv.org/abs/2310.11511
*Self-reflective RAG. Compare to your grounding score / immune system approach.*

ğŸ“Œ **RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval**
Sarthi et al. (2024)
*Hierarchical chunking. Compare to your markdown-aware / PDF-aware chunking.*

---

## 11. Embodied Cognition (Section 34b, 31c)

For the body awareness / somatic layer arguments.

ğŸ“– **Philosophy in the Flesh: The Embodied Mind**
Lakoff & Johnson (1999)
*Cognition is fundamentally embodied. Your argument that Monica needs hardware awareness (body) to have richer cognition.*

ğŸ“Œ **The Embodied Mind: Cognitive Science and Human Experience**
Varela, Thompson, Rosch (1991)
*Enactivism â€” cognition arises through interaction with environment. Monica's development through conversation IS enactivist.*

---

## 12. Mycelial Networks (Section 8.6)

Your metaphor deserves scientific backing.

ğŸ“– **Mycelium Running: How Mushrooms Can Help Save the World**
Stamets (2005)
*The popular science source. Mycelial intelligence, nutrient transfer, immune response.*

ğŸ“– **The Wood Wide Web**
Simard et al. (1997, and Simard's 2021 "Finding the Mother Tree")
*Original research on mycorrhizal networks sharing nutrients between trees. Your swarm architecture IS a wood wide web.*

ğŸ“Œ **Intelligence Without Brains**
Various papers on slime mold (Physarum) problem-solving
*Distributed intelligence without central nervous system. Supports your thesis that cognitive architecture doesn't require centralization.*

---

## Suggested Reading Order

**Week 1 (Foundations):**
1. CoALA (Sumers et al.) â€” your theoretical framework
2. LLMs Get Lost (Laban et al.) â€” your problem statement
3. MemGPT (Packer et al.) â€” your closest comparison

**Week 2 (Psychology):**
4. Schwartz â€” IFS / 8 C's (the book, or at minimum a thorough summary)
5. Diekelmann & Born â€” sleep and memory
6. Kahneman â€” fast/slow thinking

**Week 3 (Philosophy):**
7. Butlin et al. â€” consciousness report card
8. Nagel â€” "What Is It Like to Be a Bat?"
9. Searle â€” Chinese Room (know the counterarguments too)

**Week 4 (Development & Embodiment):**
10. Generative Agents (Park et al.) â€” AI memory + reflection
11. Piaget â€” developmental stages
12. Lakoff & Johnson â€” embodied cognition

**Ongoing:** Scan the Agent-Memory-Paper-List GitHub repo weekly for new relevant papers.

---

## Papers You Could Write First (Smaller Scope)

If the full white paper feels like a mountain, these are publishable standalone:

1. **"Persistent Facts as Multi-Turn Degradation Mitigation"** â€” replicate Laban et al. on mycoSwarm, measure improvement. Tight, empirical, publishable.
2. **"IFS-Derived Self-Monitoring for LLM Agents"** â€” 8 C's vitals system alone. Novel, no one has done this.
3. **"Wu Wei Timing Gate: Contextual Response Calibration Without Additional LLM Calls"** â€” small, clean, implementable by others.

Each of these could be a workshop paper AND an InsiderLLM article.
